Exam Code: CLF - C01

Exams
1: aws certified cloud practitioner
2: aws certified solutions architect
3: aws certified sysops administrator
4: aws certified developer

==========================================================================================================================
Cloud Computing

- Cloud Computing is the on-demand delivery of compute power, database storage, applications, and other IT resources with pay-as-you-go pricing model.

- Deployment Models of the Cloud
* Private Cloud - Cloud services used by a single organization, not exposed to the public. Example: Rackspace
* Public Cloud - Cloud resources owned and operated by a third party cloud service provider delivered over the internet. Example: AWS, Azure
* Hybrid Cloud - Keep some servers on premises and extend some capabilities to the cloud.

5 characteristics of cloud computing
- On-demand self-service
- Broad network access
- Multi-tenancy and resource pooling
- Rapid elasticity and scalability
- Measured service

6 advantages of cloud computing
- Reduced Total Cost of Ownership(TCO) and Operational Expense
- Prices are reduced as AWS is more efficient due to large scale
- Scale based on actual measured usage
- Increase speed and agility
- Stop spending money running and maintaining data centers
- Go global in minutes

Types of Cloud Computing
- Infrastructure as a Service (IaaS)
- Platform as a Service (PaaS)
- Software as a Service (SaaS)

On-Premises --> Applications, Data, Runtime, Middleware, OS, Virtualization, Servers, Storage, Networking
IaaS ---------> Applications, Data, Runtime, Middleware, OS (Example: AWS EC2, Azure, Digital Ocean)
PaaS ---------> Applications, Data (Example: AWS Elastic Beanstalk, Heroku, Google App Engine)
SaaS ---------> 

============================================================================================================================
How to choose an AWS Region?
- Compliance with data governance and legal requirements.
- Proximity to customers
- Available services within a region
- Pricing

A region is comprised of multiple availability zones

Shared Responsibility Model
- Customer is responsible for - Security in the cloud, it's customer data, platform, IAM, OS, Network and Firewall configuration
- AWS is responsible for - Security of the cloud, hardware, regions etc

===========================================================================================================================
IAM - Identity and Access Management

When you create an AWS account, you are actually creating a root account. One should never use the root account.
Create users/groups within IAM with restricted access. Users/groups can be assigned IAM policies. These policies define the permissions
of the users. In AWS you apply the least privilage principle, don't give more permissions than a user needs.

To login as an IAM user, you need the AWS account id/AWS account alias, user-id, and password. If MFA is enabled, even MFA key.

Always set a Password Policy for all IAM users to ensure good security.

MFA - Multi Factor Authentication
MFA device options
1: Virtual MFA device(Google Authenticator, Authy)
2: Universal 2nd Factor(U2F) Security device (YubiKey by Yubico) - this is a physical key
3: Hardware Key Fob MFA device
4: Hardware Key Fob MFA device for AWS GovCloud(US)

MFA can be set under "My Security Credentials" page

How to access your AWS?
1: AWS Management Console (protected by password + MFA)
2: AWS Command Line Interface: CLI (protected by access keys)
3: AWS Software Development Kit: SDK - for code (protected by access keys)

You could install AWS CLI from aws on your system, open command prompt and run the following command to verify the installation.
aws --version

If you have permission issues (like I have on my company laptop), you can use AWS CloudShell 
This is a browser simulation of a command prompt which has aws cli, node and other things pre-installed.
You can practice your cli skills on it.

- IAM Roles
Some AWS service will need to perform actions on your behalf.
Ex: If Lambda needs to reach out to S3, lambda needs permissions, these permissions are called IAM Roles

- IAM Security tools
1: IAM Credentials Report (account-level)
2: IAM Access Advisor (user-level)

-IAM Best Practices
1: Do not use the root account, except for AWS account setup
2: One Physical user = One AWS user
3: Assign users to groups and assign permissions to groups
4: Create a strong password policy
5: Use and enforce MFA
6: Create and use Roles for giving permissions to AWS services
7: Use Access Keys for Programmatic Access (CLI/SDK)
8: Audit permissions of your account with IAM Credentials Report

- IAM Shared Responsibility Model
AWS - Infrastructure, Configuration and Vulnerability analysis, compliance validation
YOU - Users, Groups, Roles, Policies and monitoring

=================================================================================================================================================================
AWS Budget Setup

For billing information, navigate to "My Billing Dashboard"
By default IAM users do not have access to billing information.
The access needs to be given by the root user by activating "Activate IAM Access", this will ensure that users who are admins have access to billing information.

One can setup a budget and a budget threshold for your aws usage and aws will notify you when the expense reaches the threshold.

=================================================================================================================================================================
AWS EC2

EC2 = Elastic Compute Cloud
Infrastructure as a Service
 
- Renting virtual machines (EC2)
- Storing data on virtual drives (EBS)
- Distributing load across machines (ELB)
- Scaling the services using an auto-scaling group (ASG)

EC2 User Data
It is possible to bootstrap our instances using an EC2 User Data script.
The script is only run once at the instance first start.
EC2 user data is used to automate boot tasks such as installing updates, installing software etc

To launch an EC2 instance
1: Choose an Amazon Machine Image (AMI)
2: Choose an Instance Type
3: Configure instance details (user data, iam roles)
4: Add storage
5: Add tags
6: Configure Security Group (adding http rule to your instance)

EC2 Instance Types
1: General Purpose 
   Example: t2.micro
   Great for a diversity of workloads such as web servers or code repositories
   Balance between compute, memory, and networking
2: Compute Optimized
   Example: c5.xlarge
   Great for compute-intensive tasks that require high performance processors: batch processing workloads, media transcoding, high performance web servers,
   high performance computing, scientific modeling, machine learning, dedicated gaming servers
3: Memory Optimized
   Example: r5a.4xlarge
   Fast performance for workloads that process large data sets in memory
   Use Cases: High performance relational/non-relational databases, distributed web scale cache stores, in-memory databases optimized for BI(business intelligence)
   applications performing real-time processing of big unstructured data
4: Storage Optimized
   Example: d3.8xlarge
   Great for storage-intensive tasks that require high, sequenial read and write access to large data sets on local storage.
   Use cases: High freguency online transaction processing (OLTP) systems, Relational and NoSQL databases, cache for in-memory databases(redis), data warehousing
   applications, distributed file systems
   
- EC2 Security Groups
Security groups control how the traffic is allowed into or out of your EC2 instances.
Security groups are acting as a firewall on EC2 instances
They regulate access to ports, authorised IP ranges IPv4 and IPv6, control the inbound and outbound network

Classic Ports to know
Port 22 = SSH (Secure Shell) log into a linux instance
Port 21 = FTP (File Transfer Protocol) upload files into a file share
Port 22 = SFTP (Secure File Transfer Protocol) upload files using SSH
Port 80 = HTTP access unsecured websites
Port 443 = HTTPS access secured websites
Port 3389 = RDP (Remote Desktop Protocol) log into a Windows instance

The easiest way to connect to your linux instance is EC2 Instance Connect using the web browser.
Instance connect uses the SSH protocol to connect to your EC2 instance and opens a terminal in your browser(very cool!).
One could also ssh from their windows systems directly using the following command

ssh -i {PATH_TO_PEM_FILE} ec2-user@{PUBLIC_IP_OF_INSTANCE}

pem file is generated when you create a key pair.

DO NOT RUN aws configure command in your ec2 instance and enter your IAM credentials, instead attach an IAM Role to the EC2 instance.

- EC2 Instance Purchasing Options
1: On-Demand Instances: short workload, predictable pricing
   - Pay for what you use
   - Has the highest cost but no upfront payment
   - No long-term commitment
   - Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave
2: Reserved Instances: Minimum 1 year, upto 75% discount compared to On-demand
   - Reserved Instances: long workloads, cannot change instance type for the commited period.
   - Convertible Reserved Instances: long workloads with flexible instances types, can change the instance type if you want to. Up-to 54% discount.
   - Scheduled Reserved Instances: every Thursday between 3 and 6pm
3: Spot Instances: short workloads, cheap, can lose instances (less reliable), upto 90% discount. Useful for workloads that are resilient to failure. Ex: batch jobs, data      analysis, image processing, any distributed workloads.
4: Dedicated Hosts: book an entire physical server, control instance placement. compliance requirements and use your existing server-bound software licenses. Most expensive.

- EC2 Shared Responsibility Model
AWS is responsible for infrastructure, replacing faulty hardware, compliance validation
User is responsible for security groups rules, operating-system patches and updates, software and utilities installed on the EC2 instance, IAM roles assigned to the ec2 instance, data security on your instance.

=============================================================================================================================================================================
EC2 Instance Storage

An EBS(Elastic Block Store) Volume is a network drive you can attach to your instances while they run.
It allows your instances to persist data, even after their termination.
They can only be mounted to one instance at a time.
It uses the network to communicate with the instance, which means there might be a bit of latency.
It can be detached from an EC2 instance and attached to another one quickly
Its locked to an Availability Zone, to move a volume you need to take a snapshot of it
One needs to provision the EBS Volume beforehand by telling how many Gbs and IOPS(Input Output per second) one needs.

When an EC2 instance is created, the user is prompted with a choice related to the EBS Volume
"Delete on Termination" if one checks this box, the EBS Volume attached to the EC2 instance will get deleted once the instance is terminated.
The root volume attached to the instance has the delete on termination selected by default, however if you attach additional ebs volumes apart from 
the root volume, they are not deleted when the instance is terminated.

EBS Snapshots
Take a backup of your EBS volume at any point in time.
Not necessary to detach volume to do snapshot, but recommended.
Once the snapshot is created, you must copy the snapshot to a desired region/Availability Zone(AZ)

AMI Overview
Amazon Machine Image is the customization of an EC2 instance. You add your own software, configuration, operating system, monitoring.
AMI's are built for a specific region
AMI is also referred to as Image in AWS world
You can launch EC2 instances from:
- A public AMI: AWS Provided
- Your own AMI: you make and maintain them yourself
- An AWS Marketplace AMI: an AMI someone else made

You could launch an EC2 instance using an AMI, customize it even further, create a customized AMI from that instance and launch another instance using the 
customized AMI. This helps the user avoide doing the same customizations multiple times in multiple instances.
 
EC2 Image Builder
Used to automate the creation of Virtual Machines or Container Images
ie: automate the creation, maintain, validated and test EC2 AMI's
EC2 image builder launches a Builder EC2 Instance, create a new AMI, launches a Test EC2 Instance and test the AMI, AMI is distributed to multiple regions.

EC2 Instance Store
EBS volumes are network drives with good but "limited" performance
If you need a high-performance hardware disk, use EC2 Instance Store.
EC2 Instance Store loses its storage once the instance is terminated.

EFS - Elastic File System
This is a Network File System(NFS) that can be mounted on 100s of EC2 instances.
EFS works with Linux EC2 instances in multi-AZ
EFS drives can be mounted to multiple instances in various availability zones.

EFS Infrequent Access (EFS-IA)
Storage class that is cost-optimized for files not accessed every day.
Upto 92% lower cost compared to EFS Standard
EFS will automatically move your files to EFS-IA based on the last time the file was accessed if you enable EFS-IA with a lifecycle policy.

Shared Responsibility Model for EC2 Storage
AWS: infrastructure, replacing faulty hardware, ensuring their employees cannot access your data.
User: Setting up backup/snapshot procedures, setting up data encryption, responsibility of any data on the drives, understanding the risk of using ec2 instance store.

Amazon FSx
Launch 3rd party high-performance file systems on AWS
Fully Managed Service
3 options
1: FSx for Lustre, a fully managed, high-performance, scalable file storage for High Performance Computing (HPC)
2: FSx for Windows File Server, supports SMB protocol and Windows NTFS
3: FSx for NetApp ONTAP

=================================================================================================================================================
ELB (Elastic Load Balancing) and ASG (Auto Scaling Group)

There are two kinds of scalability
1: Vertical Scalability - Vertical Scalability means increasing the size of the instance, ex: t2.micro to t2.large, use case: database
2: Horizontal Scalability - Horizontal Scalability means increasing the number of instances / systems for your application use case: web application

High Availability means running your application/system in atleast 2 availibility zones.

Scalability: ability to accommodate a larger load by making the hardware stronger(scale up) or by adding nodes(scale out)
Elasticity: once a system is scalable, elasticity means that there will be some "auto-scaling" so that the system can scale based on the load. This is 
            cloud-frindly: pay-per-use, match demand and optimize costs
            
Load Balancing
Load Balancers are servers that forward internet traffic to multiple servers(EC2 Instances) downstream.
Use Cases: Expose a single point of access (DNS) to your application, seamlessly handle failures of downstream instances

4 kinds of load balancers offered by AWS
- Application Load Balancer (HTTP/HTTPS only) - Layer 7
- Network Load Balancer (ultra-high performance, allows for TCP) - Layer 4
- Classic Load Balancer (slowly retiring) - Layer 4 & 7
- Gateway Load Balancer 

Auto Scaling Group
The goal of Auto Scaling Group(ASG) is to scale out (add EC2 instances) to match an increased load or scale in (remove EC2 instances) to match
a decreased load. This ensures we have a minimum and maximum number of machines running. Automatically register these new instances to a load balancer.
It also replaces unhealthy instances.
It is cost saving too, because it ensures to only run at an optimal capacity.

In ASG we set the following
- minimum size
- actual size / desired capacity
- maximum size

ASG Strategies
- Manual Scaling: Update the size of an ASG manually
- Dynamic Scaling: 
   - Simple / Step Scaling: When a CloudWatch alarm is triggered (example: CPU > 70%) then add 2 units or when CPU < 30% remove 1 unit
   - Target Tracking Scaling: I want the average ASG CPU to stay at around 40%
   - Scheduled Scaling: Anticipate a scaling based on known usage patterns, ex: increase min capacity to 10 at 5pm on Fridays
- Predictive Scaling: Uses Machine Learning to predict future traffic ahead of time.

========================================================================================================================================
 - S3 (Simple Storage Service)
 Use Cases
 - Backup and storage
 - Disaster Recovery
 - Archive
 - Hybrid Cloud Storage
 - Application hosting
 - Media hosting
 - Data lakes & big data analytics
 - Software delivery
 - Static website
 
 Amazon S3 allows people to store objects (files) in buckets (directories)
 Buckets must have a globally unique name (across all regions all accounts)
 Buckets are defined at the region level
 S3 looks like a global service but buckets are created in a region
 Objects have a key
 The key is composed of prefix + object name
 
 s3://my-bucket/{prefix}/{object name}
 s3://my-bucket/my_folder/another_folder/my_file.txt
 
 There's no concept of "directories" within buckets, just keys with very long names that contain slashes ("/")
 
 Max Object Size is 5TB (5000 GB)
 If uploading more than 5GB, must use "multi-part upload"
 
 S3 Security
 - User based: IAM policies; which API calls should be allowed for a specific user from IAM console.
 - Resource based: 
     1: Bucket policies; bucket wide rules from the S3 console - allows cross account 
     2: Object Access Control List (ACL) 
     3: Bucket Access Control List (ACL)
 - Encryption: encrypt objects in Amazon S3 using encryption keys
 
 S3 Bucket policies - JSON based policies
 Use S3 bucket policy to grant public access to the bucket, force objects to be encrypted at upload, grant access to another account (cross account)
 
S3 Versioning
You can version your files in Amazon S3
It is enabled at the bucket level
It is best practice to version your buckets
 - Protect against unintended deletes (ability to restore a version)
 - Easy roll back to previous version
 
S3 Access Logs
Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket.
Very helpful to understand the root cause of an issue, or audit usage, view suspicious patterns etc

S3 Replication
Cross Region Replication (CRR) - compliance, lower latency access, replication across accounts
Same Region Replication (SRR) - live replication between production and test accounts
Must enable versioning is source and destination
Copying is asynchronous

S3 Storage Classes
- Amazon S3 Standard - General Purpose
    99.99% Availability
    Used for frequently accessed data
    low latency and high throughput
    sustain 2 concurrent facility failures
- Amazon S3 Standard - Infrequent Access (IA)
    Suitable for data that is less frequently accessed, but requires rapid access when needed
    99.9% Availability
    Lower cost compared to Amazon S3 standard, but retrieval fee
    sustain 2 concurrent facility failure
- Amazon S3 One Zone - Infrequent Access (IA)
    Same as IA but data is stored in a single AZ
    99.5% Availability
    low latency and high throughput performance
    lower cost compared to S3-IA (by 20%)
- Amazon S3 Intelligent Tiering
    99.9% Availability
    Same low latency and high throughput performance of S3 Standard
    Cost-optimized by automatically moving objects between two access tiers based on changing access patterns   
- Amazon Glacier
    Low cost object storage (in GB/month) meant for archiving/backup
    data is retained for the longer term
    various retrieval options of time + fees for retrieval 
       - Expedited (1 to 5 minutes)
       - Standard (3 to 5 hours)
       - Bulk (5 to 12 hours)
- Amazon Glacier Deep Archive
    Cheapest of them all
    various retrieval options of time + fees for retrieval
       - Standard (12 hours)
       - Bulk (48 hours)

S3 Durability: If you store 10,000,000 objects with Amazon S3, you can on an average expect to incur a loss of a single object once every 10,000 years
S3 Availability: S3 standard has 99.99% availability, which means it will not be available 53 minutes a year

S3 Object Lock and Glacier Vault Lock
WORM Model (Write Once Read Many): Block an object version deletion for a specified amount of time

S3 Encryption
- No Encryption
- Server-side Encryption: S3 will encrypt the file
- Client-side Encryption: User encrypts the file before uploading it

Shared Responsibility Model of S3
AWS: Infrastructure (global security, durability, availability, sustain concurrent loss of data in two facilities), configuration, compliance validation
User: S3 Versioning, S3 Bucket Policy, S3 Replication Setup, S3 Storage Class, Data encryption

AWS Snow Family
Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS
Data Migration: 
    - Snowcone
    - Snowball Edge
    - Snowmobile
Edge Computing: 
    - Snowcone
    - Snowball Edge
    
AWS Snow Family: offline devices to perform data migrations
If it takes more than a week to transfer over the network, use Snowball devices.
Load the data to these devices and ship it back to AWS.

Snowball Edge
   - Snowball Edge Storage Optimized: 80TB of HDD capacity for block volume and S3 compatible object storage
   - Snowball Edge Compute Optimized: 42TB of HDD capacity for block volume and S3 compatible object storage
   Migration Size: Up to petabytes

Snowcone: Small, portable, rugged and secured, withstands harsh environments. 8TB of usable storage
Can be sent back to AWS offline or connect it to internet and use AWS DataSync to send data
Migration size: Up to 24TB

Snowmobile: Transfer exabytes of data (1EB = 1000PB = 10,00,000TB)
Each Snowmobile has 100PB of capacity
Migration Size: up to exabytes

Snow Family - Usage Process
- Request Snowball devices from the AWS console for delivery
- Install the snowball client / AWS OpsHub on your servers
- Connect the snowball to your servers and copy files using the client
- Ship back the device when you're done
- Data will be loaded into an S3
- Snowball is completely wiped

Edge Computing
Process data while its being created on an edge location (anything that doesn't have internet or far from cloud ex a ship on the sea)
Snowball Edge / Snowcone can be used for edge computing
You process the data at the place where its being created.
Snowcone: 2CPU's, 4GB of memory, wired or wireless access. USB-C power using a cord or the optional battery
Snowball Edge - Compute Optimized: 52vCPU's, 208GB RAM, Optional GPU, 42TB usable storage
Snowball Edge - Storage Optimized: Upto 40vCPUs, 80GB of RAM

AWS OpsHub is a software you install to your computer to manage your snow family device
- Transferring files
- Launching and managing instances running on snow family devices

AWS Storage Cloud Native Options
- Block: Amazon EBS, EC2 Instance Store
- File: Amazon EFS
- Object: AWS S3, Glacier

AWS Storage Gateway
Bridge between on-premise data and cloud data in S3
Types of Storage Gateway:
- File Gateway
- Volume Gateway
- Tape Gateway

============================================================================================================================
Databases in AWS
If you want to structure your data, you need to use databases
You can build indexes to efficiently query/search through the data

Relational Databases: Use SQL language to perform queries/lookups
NoSQL Databases: More like documents (JSON)

Benifits of using a managed database on AWS
- Quick provisioning, High Availability, Vertical and Horizontal Scaling
- Automated Backup and Restore, Operations, Upgrades
- Operating System Patching is handled by AWS
- Monitoring, alerting

AWS RDS
RDS stands for Relational Database Service
It's a managed Db service that uses SQL as the query language.
It allows you to create databases in the cloud that are managed by AWS
 - Postgres
 - MySQL
 - MariaDB
 - Oracle
 - Microsoft SQL Server
 - Aurora (AWS Proprietary database)
 
One cannot SSH into your RDS instances

AWS Aurora
Aurora is a proprietary technology from AWS (not open sourced)
PostgreSQL and MySQL are both supported as Aurora DB
Aurora is "AWS Cloud Optimized" and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS
Aurora storage automatically grows in increments of 10GB, upto 64TB
Aurora costs more than RDS (20% more)

RDS and Aurora are the two ways to set-up managed relational databases in AWS.

RDS Deployment
- Read Replicas: Scale the read workload of your DB, you can create upto 5 read replicas. Data is only written to the main db
- Multi-AZ: High availabilty. One creates a failover db in another AZ
- Multi-Region: Main db in asia, read replicas across US regions. Disaster reovery in case of region issue. Local performance for global reads.


Amazon ElastiCache
ElastiCache is to get managed Redis or Memcached
Caches are in-memory databases with high performance, low latency
Helps reduce load off databases for read intensive workloads
AWS takes care of OS maintenance/patching, optimizations, setup, configuration, monitoring, failure recovery and backups

DynamoDB
Is a fully managed, highly available NoSQL database with replication across 3 AZ's
Scales to massive workloads, distributed "serverless" database
Millions of requests per second, trillions of row, 100s of TB of storage.
Fast and consistent in performance
Single-digit millisecond latency - low latency retrieval
Integrated with IAM for security, authorization and administration
Low cost and auto scaling capabilities

DynamoDB Accelerator - DAX
Fully managed in-memory cache for DynamoDB
10x performance improvement
Secure, highly scalable and highly available
DAX is only used for and is integrated with DynamoDB, while ElastiCache can be used for other databases

DynamoDB - Global Tables
Make a DynamoDB table accessible with low latency in multiple-regions.

Redshift
Redshift is based on PostgreSQL but its not used for OLTP(online transaction processing)
Its used for OLAP(online analytical processing) analytics and data warehousing
Load data once every hour, not every second
10x better performance than other data warehouses, scale to PBs of data
Columnar storage of data (instead of row based)
Massively Parallel Query Execution(MPP), highly available
Pay as you go based on the instances provisioned
Has a SQL interface for performing the queries
BI tools such as AWS Quicksight or Tableau integrate with it

Amazon EMR - Elastic MapReduce
EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data
The clusters can be made of hundreds of EC2 instances
Also supports Apache Spark, HBase, Presto etc
EMR takes care of all the provisioning and configuration of the EC2 instances
Auto-scaling and integrated with Spot Instances
Use Cases: data processing, machine learning, web indexing, big data

Amazon Athena
Serverless query service to perform analytics against S3 objects
Analyse data in S3 using serverless SQL, use Athena
Uses standard SQL language to query the files
Supports CSV, JSON etc
Pricing: $5.00 per TB of data scanned, use compressed or columnar data for cost-savings

Amazon QuickSight
Serverless machine learning powered business intelligence service to create interactive dashboards
Fast, automatically scalable, embeddable, with per-session pricing
Integrated with RDS, Aurora, Athena, Redshift, S3

DocumentDB
Aurora is an AWS implementation of PostgreSQL/MySQL
DocumentDB is the same implementation for MongoDB(NoSQL Database)
MongoDB is used to store, query and index JSON data
Fully managed, highly available with replication across 3 AZ's
DocumentDB storage automatically grows in increments of 10GB, upto 64TB
Automatically scales to workloads with millions of requests per second.

Amazon Neptune
Fully managed graph database
A popular graph dataset would be a social network
Highly available across 3 AZ's, with upto 15 read replicas
Build and run applications working with highly connected datasets - optimized for complex and hard queries
Great for knowledge graphs (Wikipedia), fraud detection, recommendation engines, social networking

Amazon QLDB
QLDB stands for Quantum Ledger Database
A ledger is a book recording financial transactions
Used to review history of all the changes made to your application data over time.
Immutable system: no entry can be removed or modified, cryptographically verifiable
QLDB is not decentralized, this is in accordance with financial regulation rules

Amazon Managed Blockchain
Blockchain makes it possible to build applications where multiple parties can execute transactions without the need for
a trusted, central authority.
Used to
   - Join public blockchain networks
   - Create your own scalable private network
Compatible with frameworks: Hyperledger Fabric and Ethereum

DMS - Database Migration Service
Quickly and securely migrate databases to AWS
The source database remains available during the migration
Supports: 
   - Homogeneous migrations: Ex: Oracle to Oracle
   - Heterogeneous migrations: Ex: Microsoft SQL Server to Aurora
   
AWS Glue
Managed extract, transform, and load (ETL) service
Useful to prepare and transform data for analytics
Fully serverless service

S3 ---------->
               ---- Extract ----> AWS Glue -----> Transforms -------> Loads ------> Redshift
Amazon RDS--->

The AWS Glue Data Catalog is a central repository to store structural and operational metadata for all your data assets. 
For a given data set, you can store its table definition, physical location, add business relevant attributes, as well as 
track how this data has changed over time.

===============================================================================================================================================
Other Compute Options in AWS

Docker: Is a software development platform to deploy apps. 
        Apps are packaged in containers that can be run on any OS
        Apps run the same, regardless of where they're run
        Scale containers up and down very quickly
        
ECS
Elastic Container Service
Launch Docker containers on AWS
You must provision and maintain the infrastucture(the EC2 instances)

Fargate
Launch Docker containers on AWS
You do not provision the infrastructure(no EC2 instances to manage)
Serverless offering
AWS just runs containers for you based on the CPU/RAM you need

ECR
Elastic Container Registry
Private Docker Registry on AWS
This is where you store your Docker images so they can be run by ECS or Fargate

Lambda
Virtual functions - no servers to manage
Limited by time - short executions
Run on-demand
Scaling is automated

Benefits of Lambda
   - Pay per request and compute time
   - Integrated with the whole AWS suite of services
   - Event-Driven: functions get invoked by AWS when needed
   - Easy to get more resources per function
   - Increasing RAM will also improve CPU and network
   
Lambda allows us to create CRON jobs
Lambda pricing is based on calls and duration

AWS API Gateway
Fully managed service for developers to easily create, publish, maintain, monitor and secure API's
Serverless and scalable
Supports RESTful APIs and WebSocket APIs
Example: Building a serverless API

AWS Batch
Fully managed batch processing at any scale
Efficiently run 100,000s of computing batch jobs on AWS
A batch job is a job with a start and an end
Batch will dynamically launch EC2 instances or Spot Instances
AWS Batch provisions the right amount of compute/memory
You submit or schedule batch jobs and AWS Batch does the rest
Batch jobs are defined as Docker images and run on ECS

Batch vs Lambda
Lambda: 
      - Time limit
      - Limited runtime
      - Limited temporary disk space
      - Serverless
Batch:
      - No time limit
      - Any runtime as long as its packaged as a Docker image
      - Rely on EBS/instance store for disk space
      - Relies on EC2
  
Amason Lightsail
Virtual servers, storage, databases and networking
Simpler alternative to using EC2, RDS, ELB, EBS, Route 53
Great for people with little cloud experience
Has high availability but no auto-scaling, limited AWS integrations

===================================================================================================================
Deploying and Managing Infrastructure at Scale Section

CloudFormation
CloudFormation is a declarative way of outlining your AWS Infrastructure, for any resources
Example: within the CloudFormation template, you say
         - I want a security group
         - I want two EC2 instances using this security group
         - I want an S3 bucket
         - I want a load balancer in front of these machines
         
Then CloudFormation creates those for you, in the right order, with the exact configuration that you specify
Benefits of AWS CloudFormation
 - Infrastructure as Code
   No resources are manually created, which is excellent for control
   Changes to the infrastructure are reviewed through code
 - Cost
   Each resources within the stack is tagged with an identifier so you can easily see how much a stack costs you
   You can estimate the costs of your resources using the CloudFormation template
   Savings Strategy: In Dev, you could automatically delete the templates at 5PM and recreated at 8AM safely
 - Productivity
   Ability to destroy and re-create an infrastructure on the cloud on the fly
   Declarative programming
 - Don't re-invent the wheel
   Leverage existing templates on the web
 - Supports (almost) all AWS resources
 
 AWS CDK (Cloud Development Kit)
 Define your cloud infrastructure using a familiar language: Ex JavaScript, .NET, Python
 The code is complied into a CloudFormation template
 You can thereforce deploy infrastructure and application runtime code together

AWS Elastic Beanstalk
- Elastic Beanstalk is a developer centric view of deploying an application on AWS
- It uses all the components we've seen before EC2, ASG, ELB, RDS etc but its all in one view that's easy to make sense of
- We still have full control over the configuration
- Beanstalk = Platform as a Service (PaaS)
Responsibilty of the user: Just the application code is the responsibility of the developer
Three Architecture models
   - Single Instance deployment: good for dev
   - LB + ASG: great for production or pre-prodction web applications
   - ASG only: great for non-web apps in production (workers etc)
Beanstalk also has a dashboard to monitor the health of the applications

AWS CodeDeploy
- We want to deploy our application automatically
- Works with EC2 instances and On-Premises Servers (Hybrid service)

AWS CodeCommit
Developers usually store code in a repository, using the Git technology
A famous public offering is GitHub, AWS's competing product is CodeCommit
Source-control service that hosts Git-based repositories
The code changes are automatically versioned
Private, secured and integrated with AWS
Fully managed

AWS CloudBuild
Code building service in the cloud.
Compiles source code, run tests, and produces packages that are ready to be deployed (by CodeDeploy for example)
Fully managed, serverless
Continuously scalable and highly available
Secure
Pay-as-you-go pricing - only pay for the build time

AWS CodePipeline
Orchestrate the different steps to have the code automatically pushed to production
Code -> Build -> Test -> Provision -> Deploy
Basis for CICD (Continuous Integration & Continuous Delivery)
Ex: CodeCommit -> CodeBuild -> CodeDeploy -> Elastic Beanstalk

AWS CodeArtifact
Software packages have code dependencies.
Storing and retrieving these dependencies is called artifact management.
CodeArtifact is a secure, scalable, cost-effective artifact management for software development
Works with common dependency management tools such as Maven, Gradle, npm, yarn etc

AWS CodeStar
Unified UI to easily manage software development activities in one place.
Quick way to get started to correctly set-up CodeCommit, CodePipeline, CodeBuild, CodeDeploy, Elastic Beanstalk, EC2 etc

AWS Cloud9
AWS Cloud9 is a cloud IDE(Integrated Development Environment) for writing, running and debugging code
A cloud IDE can be used within a web browser, meaning you can work on your projects from your office, home, or anywhere with internet.
AWS Cloud9 also allows for code collaboration in real-time.

AWS Systems Manager(SSM)
Helps you manage your EC2 and On-Premises systems at scale
Another Hybrid AWS service
Most important features:
   - Patching automation for enhanced compliance
   - Run commands across an entire fleet of servers
   - Store parameter configuration with the SSM Parameter Store
Works for both Windows and Linux OS
We need to install SSM agent onto the systems we control

Systems Manager(SSM) Session Manager feature
Allows you to start a secure shell on your EC2 and on-premises servers
No SSH access, bastion hosts or SSH keys needed

AWS OpsWorks
Chef and Puppet help you perform server configuration automatically, or repetitive actions.
They work great with EC2 and On-premises VM
AWS OpsWorks = Managed Chef and Puppet

=====================================================================================================================
AWS Route 53
Route 53 is a managed DNS(Domain Name System)
Most common records
- www.google.com => 12.34.56.78 == A record (IPv4)
- www.google.com => 2001:0db8:t34d:agaf:87ed:hy12: == AAAA (IPv6)
- search.google.com => www.google.com == CNAME:hostname to hostname
- example.com => AWS resource == Alias (ex: ELB, CloudFront, S3 etc)

Route 53 Routing Policies
- Simple Routing Policy (No Health Check)
- Weighted Routing Policy
- Latency Routing Policy
- Falover Routing Policy (Diaster Recovery)

AWS CloudFront
CloudFront is a CDN (Content Delivery Network)
Improves read performance, by caching content in various edge locations(points of presence)
The CloudFront origin will be protected by OAI (Origin Access Identity)

S3 Transfer Acceleration
Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region

AWS Global Accelerator
Improve global application availability and performance using the AWS global network
Leverage the AWS internal network to optimize the route to your application (60% improvement)
2 Anycast IP are created for your application and traffic is sent through Edge Locations
The Edge locations send the traffic to your application

AWS Outposts
AWS Outposts are server racks that offers the same AWS infrastructure, services, API's and tools to build your own applications on-premises just as in
the cloud. AWS will setup and manage Outposts racks within your on-premises infrastructure and you can start leveraging AWS services on-premises.

AWS WaveLength
WaveLength Zones are infrastructure deployments embedded within the telecommunications providers' datacenters at the edge of 5G networks
Ultra-low latency applications through 5G networks

AWS Local Zones
Places AWS compute, storage, database, and other selected AWS services closer to end users to run latency-sensitive applications
Extend your VPC to more locations - "Extension of an AWS Region"

==========================================================================================================================================================
- Amazon SQS - Simple Queue Service

Fully managed service, used to decouple applications
Scales from 1 message per second to 10000s per second
Messages are deleted after they're read by consumers
Consumers share the work to read messages and scale horizontally

Requests ----> Web Servers ----> SQS Queue ----> Video Processing

Now the web servers service and the video processing service are decoupled and the important thing here is the EC2 instances for Video Processing
can be independently scaled without touching the Web Servers EC2 instances.

- Amazon SNS - Simple Notification Service
The "event publishers" only sends message to one SNS topic while we can have as many "event subscribers" as we want to listen to the SNS topic notifications
Each subscriber to the topic will get all the messages.

- Amazon Kinesis
Real-time big data streaming
Managed service to collect, process, and analyze real-time streaming data at any scale.

- Amazon MQ = managed Apache ActiveMQ
SQS and SNS are cloud-native services using proprietary protocols from AWS.
Traditional applications running from on-premise may use open protocols such as MQTT, AMQP etc
When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, one can use Amazon MQ

===============================================================================================================================================================
